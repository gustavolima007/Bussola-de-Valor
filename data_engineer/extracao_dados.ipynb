{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d56c0ba",
   "metadata": {},
   "source": [
    "# Lista de empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba211f28",
   "metadata": {},
   "source": [
    "üîç Busca inicial Obt√©m a lista completa de ativos dispon√≠veis na BRAPI.\n",
    "\n",
    "üßπ Filtragem criteriosa\n",
    "\n",
    "Mant√©m apenas ativos dos tipos \"stock\" e \"fund\".\n",
    "\n",
    "Exclui tickers fracionados (aqueles terminados em F.SA).\n",
    "\n",
    "Considera exclusivamente ativos cujo pa√≠s de origem seja Brasil, validado via yfinance.\n",
    "\n",
    "Restringe a an√°lise a setores espec√≠ficos: Finance, Utilities, Communications e Industrial Services.\n",
    "\n",
    "üß† Valida√ß√£o e enriquecimento\n",
    "\n",
    "Usa yfinance para confirmar o pa√≠s e obter o nome completo da empresa.\n",
    "\n",
    "Traduz os nomes dos setores para o padr√£o da B3.\n",
    "\n",
    "Traduz automaticamente os subsetores de ingl√™s para portugu√™s brasileiro usando a biblioteca deep-translator.\n",
    "\n",
    "Monta um dicion√°rio com os principais atributos: ticker, empresa, setor, subsetor, tipo, logo.\n",
    "\n",
    "üíæ Exporta√ß√£o final\n",
    "\n",
    "Organiza os dados em estrutura limpa e ordenada por setor e valor de mercado.\n",
    "\n",
    "Salva os resultados em um arquivo .csv, com o caminho de sa√≠da definido dinamicamente via vari√°vel no arquivo .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [1] BBAS3.SA inclu√≠do\n",
      "‚úÖ [2] B3SA3.SA inclu√≠do\n",
      "‚úÖ [3] BBDC4.SA inclu√≠do\n",
      "‚úÖ [4] ITUB4.SA inclu√≠do\n",
      "‚úÖ [5] MOVI3.SA inclu√≠do\n",
      "‚úÖ [6] CSAN3.SA inclu√≠do\n",
      "‚úÖ [7] ELET3.SA inclu√≠do\n",
      "‚úÖ [8] ITSA4.SA inclu√≠do\n",
      "‚úÖ [9] CPLE6.SA inclu√≠do\n",
      "‚úÖ [10] VAMO3.SA inclu√≠do\n",
      "‚úÖ [11] BPAC11.SA inclu√≠do\n",
      "‚úÖ [12] RENT3.SA inclu√≠do\n",
      "‚úÖ [13] CMIG4.SA inclu√≠do\n",
      "‚úÖ [14] ENEV3.SA inclu√≠do\n",
      "‚úÖ [15] EQTL3.SA inclu√≠do\n",
      "‚úÖ [16] TEND3.SA inclu√≠do\n",
      "‚úÖ [17] CPLE3.SA inclu√≠do\n",
      "‚úÖ [18] BBSE3.SA inclu√≠do\n",
      "‚úÖ [19] AURE3.SA inclu√≠do\n",
      "‚úÖ [20] ELET6.SA inclu√≠do\n",
      "‚úÖ [21] ALOS3.SA inclu√≠do\n",
      "‚úÖ [22] BBDC3.SA inclu√≠do\n",
      "‚úÖ [23] CYRE3.SA inclu√≠do\n",
      "‚úÖ [24] CURY3.SA inclu√≠do\n",
      "‚úÖ [25] VIVT3.SA inclu√≠do\n",
      "‚úÖ [26] TIMS3.SA inclu√≠do\n",
      "‚úÖ [27] MULT3.SA inclu√≠do\n",
      "‚úÖ [28] CPFE3.SA inclu√≠do\n",
      "‚úÖ [29] BRAP4.SA inclu√≠do\n",
      "‚úÖ [30] SBSP3.SA inclu√≠do\n",
      "‚úÖ [31] CSMG3.SA inclu√≠do\n",
      "‚úÖ [32] CXSE3.SA inclu√≠do\n",
      "‚úÖ [33] SRNA3.SA inclu√≠do\n",
      "‚úÖ [34] ENGI11.SA inclu√≠do\n",
      "‚úÖ [35] SAPR11.SA inclu√≠do\n",
      "‚úÖ [36] AZTE3.SA inclu√≠do\n",
      "‚úÖ [37] PSSA3.SA inclu√≠do\n",
      "‚úÖ [38] EZTC3.SA inclu√≠do\n",
      "‚úÖ [39] JHSF3.SA inclu√≠do\n",
      "‚úÖ [40] SAPR4.SA inclu√≠do\n",
      "‚úÖ [41] PLPL3.SA inclu√≠do\n",
      "‚úÖ [42] MTRE3.SA inclu√≠do\n",
      "‚úÖ [43] ISAE4.SA inclu√≠do\n",
      "‚úÖ [44] DIRR3.SA inclu√≠do\n",
      "‚úÖ [45] PDGR3.SA inclu√≠do\n",
      "‚úÖ [46] IGTI11.SA inclu√≠do\n",
      "‚úÖ [47] TAEE11.SA inclu√≠do\n",
      "‚úÖ [48] NEOE3.SA inclu√≠do\n",
      "‚úÖ [49] EGIE3.SA inclu√≠do\n",
      "‚úÖ [50] LIGT3.SA inclu√≠do\n",
      "‚úÖ [51] OIBR3.SA inclu√≠do\n",
      "‚úÖ [52] SANB11.SA inclu√≠do\n",
      "‚úÖ [53] WIZC3.SA inclu√≠do\n",
      "‚úÖ [54] BRSR6.SA inclu√≠do\n",
      "‚úÖ [55] AZEV4.SA inclu√≠do\n",
      "‚úÖ [56] MILS3.SA inclu√≠do\n",
      "‚úÖ [57] ALUP11.SA inclu√≠do\n",
      "‚úÖ [58] MDNE3.SA inclu√≠do\n",
      "‚úÖ [59] AZEV3.SA inclu√≠do\n",
      "‚úÖ [60] BPAN4.SA inclu√≠do\n",
      "‚úÖ [61] LAVV3.SA inclu√≠do\n",
      "‚úÖ [62] ARML3.SA inclu√≠do\n",
      "‚úÖ [63] IRBR3.SA inclu√≠do\n",
      "‚úÖ [64] LOGG3.SA inclu√≠do\n",
      "‚úÖ [65] ITUB3.SA inclu√≠do\n",
      "‚úÖ [66] EVEN3.SA inclu√≠do\n",
      "‚úÖ [67] GFSA3.SA inclu√≠do\n",
      "‚úÖ [68] ORVR3.SA inclu√≠do\n",
      "‚úÖ [69] SAPR3.SA inclu√≠do\n",
      "‚úÖ [70] FIQE3.SA inclu√≠do\n",
      "‚úÖ [71] BMGB4.SA inclu√≠do\n",
      "‚úÖ [72] TCSA3.SA inclu√≠do\n",
      "‚úÖ [73] SYNE3.SA inclu√≠do\n",
      "‚úÖ [74] PRNR3.SA inclu√≠do\n",
      "‚úÖ [75] HBRE3.SA inclu√≠do\n",
      "‚úÖ [76] ABCB4.SA inclu√≠do\n",
      "‚úÖ [77] BRBI11.SA inclu√≠do\n",
      "‚úÖ [78] MELK3.SA inclu√≠do\n",
      "‚úÖ [79] BRST3.SA inclu√≠do\n",
      "‚úÖ [80] TAEE4.SA inclu√≠do\n",
      "‚úÖ [81] PINE4.SA inclu√≠do\n",
      "‚úÖ [82] AGRO3.SA inclu√≠do\n",
      "‚úÖ [83] CMIG3.SA inclu√≠do\n",
      "‚úÖ [84] ITSA3.SA inclu√≠do\n",
      "‚úÖ [85] DESK3.SA inclu√≠do\n",
      "‚úÖ [86] TAEE3.SA inclu√≠do\n",
      "‚úÖ [87] OIBR4.SA inclu√≠do\n",
      "‚úÖ [88] RNEW4.SA inclu√≠do\n",
      "‚úÖ [89] SANB3.SA inclu√≠do\n",
      "‚úÖ [90] LAND3.SA inclu√≠do\n",
      "‚úÖ [91] IGTI3.SA inclu√≠do\n",
      "‚úÖ [92] SANB4.SA inclu√≠do\n",
      "‚úÖ [93] BRAP3.SA inclu√≠do\n",
      "‚úÖ [94] ENGI4.SA inclu√≠do\n",
      "‚úÖ [95] RNEW3.SA inclu√≠do\n",
      "‚úÖ [96] BMEB4.SA inclu√≠do\n",
      "‚úÖ [98] BPAC5.SA inclu√≠do\n",
      "‚úÖ [99] ENGI3.SA inclu√≠do\n",
      "‚úÖ [100] BEES3.SA inclu√≠do\n",
      "‚úÖ [101] ALUP4.SA inclu√≠do\n",
      "‚úÖ [102] REDE3.SA inclu√≠do\n",
      "‚úÖ [103] BSLI3.SA inclu√≠do\n",
      "‚úÖ [104] SCAR3.SA inclu√≠do\n",
      "‚úÖ [105] TELB4.SA inclu√≠do\n",
      "‚úÖ [106] TELB3.SA inclu√≠do\n",
      "‚úÖ [107] INEP3.SA inclu√≠do\n",
      "‚úÖ [108] SNCI11.SA inclu√≠do\n",
      "‚úÖ [109] BAZA3.SA inclu√≠do\n",
      "‚úÖ [110] BPAC3.SA inclu√≠do\n",
      "‚úÖ [111] CLSC4.SA inclu√≠do\n",
      "‚úÖ [112] CEBR3.SA inclu√≠do\n",
      "‚úÖ [113] INEP4.SA inclu√≠do\n",
      "‚úÖ [114] ALUP3.SA inclu√≠do\n",
      "‚úÖ [115] ISAE3.SA inclu√≠do\n",
      "‚úÖ [116] AVLL3.SA inclu√≠do\n",
      "‚úÖ [117] CEBR6.SA inclu√≠do\n",
      "‚úÖ [118] BEES4.SA inclu√≠do\n",
      "‚úÖ [119] BRSR3.SA inclu√≠do\n",
      "‚úÖ [120] COCE5.SA inclu√≠do\n",
      "‚úÖ [121] CGAS5.SA inclu√≠do\n",
      "‚úÖ [122] NEXP3.SA inclu√≠do\n",
      "‚úÖ [123] RNEW11.SA inclu√≠do\n",
      "‚úÖ [124] PINE3.SA inclu√≠do\n",
      "‚úÖ [125] OSXB3.SA inclu√≠do\n",
      "‚úÖ [126] RDNI3.SA inclu√≠do\n",
      "‚úÖ [127] FSPE11.SA inclu√≠do\n",
      "‚úÖ [128] FIEI3.SA inclu√≠do\n",
      "‚úÖ [129] CEBR5.SA inclu√≠do\n",
      "‚úÖ [130] RBIF11.SA inclu√≠do\n",
      "‚úÖ [131] ADMF3.SA inclu√≠do\n",
      "‚úÖ [132] BMEB3.SA inclu√≠do\n",
      "‚úÖ [133] GEPA4.SA inclu√≠do\n",
      "‚úÖ [134] CEEB3.SA inclu√≠do\n",
      "‚úÖ [135] BGIP4.SA inclu√≠do\n",
      "‚úÖ [136] EMAE4.SA inclu√≠do\n",
      "‚úÖ [137] ENMT4.SA inclu√≠do\n",
      "‚úÖ [138] BSLI4.SA inclu√≠do\n",
      "‚úÖ [139] SOND6.SA inclu√≠do\n",
      "‚úÖ [140] CPLE5.SA inclu√≠do\n",
      "‚úÖ [141] SOND5.SA inclu√≠do\n",
      "‚úÖ [142] EKTR4.SA inclu√≠do\n",
      "‚úÖ [143] IRIM11.SA inclu√≠do\n",
      "‚úÖ [144] PPLA11.SA inclu√≠do\n",
      "‚úÖ [145] GEPA3.SA inclu√≠do\n",
      "‚úÖ [146] EQPA3.SA inclu√≠do\n",
      "‚úÖ [147] ENMT3.SA inclu√≠do\n",
      "‚úÖ [148] MOAR3.SA inclu√≠do\n",
      "‚úÖ [149] BMIN3.SA inclu√≠do\n",
      "‚úÖ [150] EGYR11.SA inclu√≠do\n",
      "‚úÖ [151] RENV11.SA inclu√≠do\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'E:\\\\Github\\\\Unicamp\\\\Bussola-de-Valor\\\\data\\x07coes_e_fundos.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     93\u001b[39m     caminho = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOUTPUT_PATH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m     os.makedirs(os.path.dirname(caminho), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìÅ Arquivo salvo em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaminho\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'E:\\\\Github\\\\Unicamp\\\\Bussola-de-Valor\\\\data\\x07coes_e_fundos.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "load_dotenv()\n",
    "\n",
    "# üéØ Setores-alvo\n",
    "SETORES_ALVO = {\n",
    "    \"Finance\",\n",
    "    \"Utilities\",\n",
    "    \"Communications\",\n",
    "    \"Industrial Services\"\n",
    "}\n",
    "\n",
    "# üåê Tradu√ß√£o de setores para padr√£o B3\n",
    "TRADUCAO_SETORES = {\n",
    "    \"Finance\": \"Financeiro\",\n",
    "    \"Utilities\": \"Utilidade P√∫blica\",\n",
    "    \"Communications\": \"Comunica√ß√µes\",\n",
    "    \"Industrial Services\": \"Servi√ßos Industriais\"\n",
    "}\n",
    "\n",
    "# üè∑Ô∏è Tradu√ß√£o de tipos de ativos\n",
    "TRADUCAO_TIPOS = {\n",
    "    \"stock\": \"A√ß√£o\",\n",
    "    \"fund\": \"Fundo\"\n",
    "}\n",
    "\n",
    "# üåç Fun√ß√£o para traduzir subsetores\n",
    "def traduzir_subsetor(texto):\n",
    "    if not texto or texto == \"N/A\":\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        return GoogleTranslator(source='en', target='pt').translate(texto)\n",
    "    except Exception:\n",
    "        return texto  # fallback para o original se falhar\n",
    "\n",
    "# üîç Fun√ß√£o principal\n",
    "def coletar_dados_ativos(setores):\n",
    "    try:\n",
    "        lista = requests.get(\"https://brapi.dev/api/quote/list\").json().get(\"stocks\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao buscar ativos: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ativos = {\n",
    "        f\"{item['stock']}.SA\": {\n",
    "            \"setor_brapi\": TRADUCAO_SETORES.get(item[\"sector\"], item[\"sector\"]),\n",
    "            \"tipo\": item[\"type\"],\n",
    "            \"logo\": item.get(\"logo\")\n",
    "        }\n",
    "        for item in lista\n",
    "        if item[\"sector\"] in setores and item[\"type\"] in {\"stock\", \"fund\"} and not item[\"stock\"].endswith(\"F\")\n",
    "    }\n",
    "\n",
    "    registros = []\n",
    "    for i, (ticker, dados) in enumerate(ativos.items(), 1):\n",
    "        try:\n",
    "            info = yf.Ticker(ticker).info\n",
    "            if info.get(\"country\") != \"Brazil\":\n",
    "                continue\n",
    "\n",
    "            subsetor_en = info.get(\"industry\", \"N/A\")\n",
    "            subsetor_pt = traduzir_subsetor(subsetor_en)\n",
    "\n",
    "            registros.append({\n",
    "                \"ticker\": ticker,\n",
    "                \"empresa\": info.get(\"longName\", \"N/A\"),\n",
    "                \"setor_brapi\": dados[\"setor_brapi\"],\n",
    "                \"subsetor_yfinance\": subsetor_pt,\n",
    "                \"pais\": info.get(\"country\", \"N/A\"),\n",
    "                \"tipo\": TRADUCAO_TIPOS.get(dados[\"tipo\"], dados[\"tipo\"]),  # Aplicando tradu√ß√£o aqui\n",
    "                \"market_cap\": info.get(\"marketCap\"),\n",
    "                \"logo\": dados[\"logo\"]\n",
    "            })\n",
    "            print(f\"‚úÖ [{i}] {ticker} inclu√≠do\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(registros)\n",
    "\n",
    "# üíæ Execu√ß√£o e salvamento\n",
    "df = coletar_dados_ativos(SETORES_ALVO)\n",
    "\n",
    "if not df.empty:\n",
    "    df[\"market_cap_num\"] = pd.to_numeric(df[\"market_cap\"], errors=\"coerce\")\n",
    "    df[\"market_cap\"] = df[\"market_cap_num\"].apply(\n",
    "        lambda x: f\"R$ {x/1_000_000_000:.2f} Bi\" if pd.notna(x) and x > 0 else \"N/A\"\n",
    "    )\n",
    "\n",
    "    df = df.sort_values(by=[\"setor_brapi\", \"market_cap_num\"], ascending=[True, False])\n",
    "    df.drop(columns=[\"market_cap_num\"], inplace=True)\n",
    "\n",
    "    caminho = os.getenv(\"OUTPUT_PATH\")\n",
    "    os.makedirs(os.path.dirname(caminho), exist_ok=True)\n",
    "    df.to_csv(caminho, index=False)\n",
    "    print(f\"\\nüìÅ Arquivo salvo em: {caminho}\")\n",
    "else:\n",
    "    print(\"\\nüö´ Nenhum ativo encontrado para os crit√©rios definidos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1b6b5",
   "metadata": {},
   "source": [
    "# Dividendos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125c1bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Github\\\\Unicamp\\\\Bussola-de-Valor\\\\data\\\\acoes_e_fundos_filtrado.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m csv_path = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mFILTERED_COMPANIES_PATH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# L√™ o CSV e extrai os tickers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m tickers = df[\u001b[33m'\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m'\u001b[39m].dropna().unique().tolist()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Define o per√≠odo de 7 anos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Github\\Unicamp\\Bussola-de-Valor\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'E:\\\\Github\\\\Unicamp\\\\Bussola-de-Valor\\\\data\\\\acoes_e_fundos_filtrado.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Carrega vari√°veis do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Caminho do arquivo CSV com os tickers\n",
    "csv_path = os.getenv(\"FILTERED_COMPANIES_PATH\")\n",
    "\n",
    "# L√™ o CSV e extrai os tickers\n",
    "df = pd.read_csv(csv_path)\n",
    "tickers = df['Ticker'].dropna().unique().tolist()\n",
    "\n",
    "# Define o per√≠odo de 7 anos\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=7*365)\n",
    "\n",
    "todos_dividendos = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        acao = yf.Ticker(ticker)\n",
    "        dividendos = acao.dividends\n",
    "\n",
    "        # Remove timezone do √≠ndice\n",
    "        dividendos.index = dividendos.index.tz_localize(None)\n",
    "\n",
    "        # Filtra dividendos dos √∫ltimos 7 anos\n",
    "        dividendos = dividendos[dividendos.index >= start_date]\n",
    "\n",
    "        if not dividendos.empty:\n",
    "            df_div = dividendos.reset_index()\n",
    "            df_div.columns = ['Data', 'Valor']\n",
    "            df_div['Ticker'] = ticker.replace('.SA', '')\n",
    "            todos_dividendos.append(df_div)\n",
    "\n",
    "            print(f\"‚úÖ Dividendos coletados para {ticker}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Sem dividendos recentes para {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar {ticker}: {e}\")\n",
    "\n",
    "# Junta tudo em um √∫nico DataFrame\n",
    "df_final = pd.concat(todos_dividendos, ignore_index=True)\n",
    "\n",
    "# Salva em um √∫nico CSV\n",
    "df_final.to_csv(\"../data/todos_dividendos.csv\", index=False)\n",
    "\n",
    "print(\"üèÅ Finalizado! Dividendos salvos em todos_dividendos.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd63b78",
   "metadata": {},
   "source": [
    "### Dividendo por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c59dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler o arquivo CSV\n",
    "df = pd.read_csv('../data/todos_dividendos.csv')\n",
    "\n",
    "# Converter a coluna 'Data' para datetime e extrair o ano\n",
    "df['Data'] = pd.to_datetime(df['Data'])\n",
    "df['ano'] = df['Data'].dt.year\n",
    "\n",
    "# Renomear colunas para min√∫sculas\n",
    "df = df.rename(columns={'Ticker': 'ticket', 'Valor': 'dividendo', 'Data': 'data'})\n",
    "\n",
    "# Somar os dividendos por ano e ticket\n",
    "soma_por_ano_ticket = df.groupby(['ano', 'ticket'])['dividendo'].sum().reset_index()\n",
    "\n",
    "# Salvar o resultado em um CSV\n",
    "soma_por_ano_ticket.to_csv('../data/dividendos_ano.csv', index=False)\n",
    "print(\"Arquivo 'dividendos_ano.csv' gerado com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "div_ano = pd.read_csv('../data/dividendos_ano.csv')\n",
    "print(div_ano.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47693105",
   "metadata": {},
   "source": [
    "### Dividendos por ano resumido (DY5 e DY12M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dividendos por ano e ticket\n",
    "df = pd.read_csv('../data/dividendos_ano.csv')\n",
    "\n",
    "# Descobrir o √∫ltimo ano dispon√≠vel\n",
    "ultimo_ano = df['ano'].max()\n",
    "\n",
    "# Dividendos dos √∫ltimos 5 anos\n",
    "div_5anos = df[df['ano'] >= ultimo_ano - 4]\n",
    "soma_5anos = div_5anos.groupby('ticket')['dividendo'].sum().reset_index()\n",
    "soma_5anos = soma_5anos.rename(columns={'dividendo': 'valor_5anos'})\n",
    "\n",
    "# Dividendos dos √∫ltimos 12 meses\n",
    "div_12m = df[df['ano'] == ultimo_ano]\n",
    "soma_12m = div_12m[['ticket', 'dividendo']].rename(columns={'dividendo': 'valor_12m'})\n",
    "\n",
    "# Juntar os dois\n",
    "resumo = pd.merge(soma_5anos, soma_12m, on='ticket', how='outer').fillna(0)\n",
    "\n",
    "# Reorganizar colunas\n",
    "resumo = resumo[['ticket', 'valor_5anos', 'valor_12m']]\n",
    "\n",
    "# Salvar resultado\n",
    "resumo.to_csv('../data/dividendos_ano_resumo.csv', index=False)\n",
    "print(\"Arquivo 'dividendos_ano_resumo.csv' gerado com sucesso!\")\n",
    "print(resumo.head())\n",
    "print(resumo.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f7389",
   "metadata": {},
   "source": [
    "## Extraindo valores das a√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando arquivo: E:\\Github\\Unicamp\\Bussola-de-Valor\\data\\acoes_e_fundos_filtrado.csv\n",
      "Processando 150 ativos encontrados no arquivo: VIVT3.SA, TIMS3.SA, FIQE3.SA, BRST3.SA, TELB4.SA...\n",
      "Baixando dados hist√≥ricos de 150 ativos...\n",
      "\n",
      "--- Exibi√ß√£o Resumida (5 primeiras linhas) ---\n",
      "         fechamento_atual\n",
      "ticker                   \n",
      "VIVT3.SA         R$ 32.92\n",
      "TIMS3.SA         R$ 22.37\n",
      "FIQE3.SA          R$ 3.92\n",
      "BRST3.SA          R$ 2.61\n",
      "TELB4.SA          R$ 7.40\n",
      "\n",
      "‚úÖ Tabela completa salva em: E:\\Github\\Unicamp\\Bussola-de-Valor\\data\\precos_acoes_completo.csv\n",
      "‚úÖ Tabela resumida salva em: E:\\Github\\Unicamp\\Bussola-de-Valor\\data\\precos_acoes.csv\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "   ‚Ä¢ Total de ativos processados: 150\n",
      "   ‚Ä¢ Per√≠odo analisado: 7 anos\n",
      "   ‚Ä¢ Total de registros na tabela completa: 1091\n",
      "   ‚Ä¢ Data de execu√ß√£o: 10/08/2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar as vari√°veis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Ignora avisos futuros para manter o output limpo\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def ler_tickers_do_csv(caminho_do_arquivo: str, coluna_ticker: str = 'ticker') -> list:\n",
    "    \"\"\"L√™ um arquivo CSV e extrai uma lista de tickers √∫nicos.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(caminho_do_arquivo)\n",
    "        if coluna_ticker not in df.columns:\n",
    "            print(f\"Erro: A coluna '{coluna_ticker}' n√£o foi encontrada no arquivo.\")\n",
    "            print(f\"Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "            return []\n",
    "        tickers = df[coluna_ticker].dropna().unique().tolist()\n",
    "        return tickers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: O arquivo n√£o foi encontrado em '{caminho_do_arquivo}'\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao ler o arquivo: {e}\")\n",
    "        return []\n",
    "\n",
    "def gerar_tabela_comparativa_precos(lista_tickers: list, anos_anteriores: int = 7) -> tuple[pd.DataFrame, pd.DataFrame] | tuple[None, None]:\n",
    "    \"\"\"Busca o pre√ßo de fechamento ajustado para uma lista de tickers.\"\"\"\n",
    "    try:\n",
    "        # Verifica se o ticker j√° tem .SA, se n√£o tiver, adiciona\n",
    "        tickers_sa = []\n",
    "        for t in lista_tickers:\n",
    "            ticker_upper = t.upper()\n",
    "            if ticker_upper.endswith('.SA'):\n",
    "                tickers_sa.append(ticker_upper)\n",
    "            else:\n",
    "                tickers_sa.append(f\"{ticker_upper}.SA\")\n",
    "        \n",
    "        hoje = date.today()\n",
    "        ano_inicio = hoje.year - anos_anteriores\n",
    "        \n",
    "        print(f\"Baixando dados hist√≥ricos de {len(tickers_sa)} ativos...\")\n",
    "        hist = yf.download(tickers_sa, start=f\"{ano_inicio}-01-01\", end=hoje, auto_adjust=True, progress=False)\n",
    "        \n",
    "        if hist.empty:\n",
    "            print(\"Nenhum dado hist√≥rico foi encontrado.\")\n",
    "            return None, None\n",
    "            \n",
    "        df_closes = hist['Close']\n",
    "        \n",
    "        # Lista para tabela completa (ticker, ano, fechamento)\n",
    "        lista_completa = []\n",
    "        # Lista para tabela resumida (ticker, fechamento_atual)\n",
    "        lista_resumida = []\n",
    "        \n",
    "        for i, ticker in enumerate(lista_tickers):\n",
    "            ticker_sa = tickers_sa[i]  # Usa o ticker j√° processado\n",
    "            \n",
    "            if ticker_sa not in df_closes.columns or df_closes[ticker_sa].dropna().empty:\n",
    "                print(f\"Aviso: N√£o foram encontrados dados para o ticker {ticker}. Pulando.\")\n",
    "                continue\n",
    "\n",
    "            # Para tabela resumida - apenas fechamento atual\n",
    "            fechamento_atual = df_closes[ticker_sa].iloc[-1].item()\n",
    "            lista_resumida.append({\n",
    "                'ticker': ticker,\n",
    "                'fechamento_atual': fechamento_atual\n",
    "            })\n",
    "            \n",
    "            # Para tabela completa - todos os anos\n",
    "            # Adicionar fechamento atual\n",
    "            lista_completa.append({\n",
    "                'ticker': ticker,\n",
    "                'ano': hoje.year,\n",
    "                'fechamento': fechamento_atual\n",
    "            })\n",
    "            \n",
    "            # Adicionar anos anteriores\n",
    "            for j in range(anos_anteriores):\n",
    "                ano_alvo = hoje.year - (j + 1)\n",
    "                \n",
    "                try:\n",
    "                    dados_ano = df_closes.loc[str(ano_alvo)]\n",
    "                    \n",
    "                    if not dados_ano.empty and not dados_ano[ticker_sa].dropna().empty:\n",
    "                        fechamento_ano = dados_ano[ticker_sa].iloc[-1].item()\n",
    "                        lista_completa.append({\n",
    "                            'ticker': ticker,\n",
    "                            'ano': ano_alvo,\n",
    "                            'fechamento': fechamento_ano\n",
    "                        })\n",
    "                except KeyError:\n",
    "                    # Ano n√£o encontrado nos dados - adiciona com None\n",
    "                    lista_completa.append({\n",
    "                        'ticker': ticker,\n",
    "                        'ano': ano_alvo,\n",
    "                        'fechamento': None\n",
    "                    })\n",
    "\n",
    "        if not lista_completa:\n",
    "            print(\"Nenhum resultado foi processado com sucesso.\")\n",
    "            return None, None\n",
    "\n",
    "        # Criar DataFrames\n",
    "        df_completo = pd.DataFrame(lista_completa)\n",
    "        df_resumido = pd.DataFrame(lista_resumida).set_index('ticker')\n",
    "        \n",
    "        return df_completo, df_resumido\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- BLOCO DE EXECU√á√ÉO PRINCIPAL ---\n",
    "\n",
    "# Pega o caminho do arquivo a partir da vari√°vel de ambiente\n",
    "csv_path = os.getenv(\"FILTERED_COMPANIES_PATH\")\n",
    "\n",
    "if csv_path:\n",
    "    print(f\"Usando arquivo: {csv_path}\")\n",
    "    \n",
    "    ativos_alvo = ler_tickers_do_csv(csv_path)\n",
    "\n",
    "    if ativos_alvo:\n",
    "        print(f\"Processando {len(ativos_alvo)} ativos encontrados no arquivo: {', '.join(ativos_alvo[:5])}...\")\n",
    "        \n",
    "        # Define o n√∫mero de anos aqui para usar depois\n",
    "        anos_para_analise = 7\n",
    "        tabela_completa, tabela_resumida = gerar_tabela_comparativa_precos(ativos_alvo, anos_anteriores=anos_para_analise)\n",
    "\n",
    "        if tabela_completa is not None and tabela_resumida is not None:\n",
    "            print(\"\\n--- Exibi√ß√£o Resumida (5 primeiras linhas) ---\")\n",
    "            \n",
    "            # Exibir apenas 5 linhas da tabela resumida com 2 casas decimais\n",
    "            tabela_exibicao = tabela_resumida.head(5).copy()\n",
    "            tabela_exibicao['fechamento_atual'] = tabela_exibicao['fechamento_atual'].apply(\n",
    "                lambda x: f\"R$ {x:.2f}\" if pd.notna(x) else \"N/D\"\n",
    "            )\n",
    "            print(tabela_exibicao.to_string())\n",
    "            \n",
    "            # Definir pasta de sa√≠da\n",
    "            output_folder = r\"E:\\Github\\Unicamp\\Bussola-de-Valor\\data\"\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            \n",
    "            # Salvar tabela completa (ticker, ano, fechamento)\n",
    "            output_path_completo = os.path.join(output_folder, \"precos_acoes_completo.csv\")\n",
    "            tabela_completa_salvar = tabela_completa.copy()\n",
    "            tabela_completa_salvar['fechamento'] = tabela_completa_salvar['fechamento'].round(2)\n",
    "            tabela_completa_salvar.to_csv(output_path_completo, index=False)\n",
    "            print(f\"\\n‚úÖ Tabela completa salva em: {output_path_completo}\")\n",
    "            \n",
    "            # Salvar tabela resumida (ticker, fechamento_atual)\n",
    "            output_path_resumido = os.path.join(output_folder, \"precos_acoes.csv\")\n",
    "            tabela_resumida_salvar = tabela_resumida.round(2)\n",
    "            tabela_resumida_salvar.to_csv(output_path_resumido)\n",
    "            print(f\"‚úÖ Tabela resumida salva em: {output_path_resumido}\")\n",
    "            \n",
    "            # Estat√≠sticas b√°sicas\n",
    "            print(f\"\\nüìä Estat√≠sticas:\")\n",
    "            print(f\"   ‚Ä¢ Total de ativos processados: {len(tabela_resumida)}\")\n",
    "            print(f\"   ‚Ä¢ Per√≠odo analisado: {anos_para_analise} anos\")\n",
    "            print(f\"   ‚Ä¢ Total de registros na tabela completa: {len(tabela_completa)}\")\n",
    "            print(f\"   ‚Ä¢ Data de execu√ß√£o: {date.today().strftime('%d/%m/%Y')}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"N√£o foi poss√≠vel gerar as tabelas de pre√ßos.\")\n",
    "    else:\n",
    "        print(\"Nenhum ativo para processar. Verifique o conte√∫do do arquivo CSV.\")\n",
    "else:\n",
    "    print(\"ERRO: A vari√°vel de ambiente 'FILTERED_COMPANIES_PATH' n√£o est√° definida no seu arquivo .env ou no sistema.\")\n",
    "    print(\"Certifique-se de que existe um arquivo .env com a linha:\")\n",
    "    print(\"FILTERED_COMPANIES_PATH=caminho/para/seu/arquivo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b65de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7017d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rever o codigo abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad630061",
   "metadata": {},
   "source": [
    "## Script de Transforma√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform.py (Vers√£o com C√°lculo de DY Padronizado)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def classify_stock_profile(price: float, market_cap: float) -> str:\n",
    "    \"\"\"\n",
    "    Classifica a a√ß√£o com base no Market Cap e Pre√ßo.\n",
    "    \"\"\"\n",
    "    if price is not None and price < 1.0:\n",
    "        return \"Penny Stock\"\n",
    "    if market_cap is None or market_cap == 0:\n",
    "        return \"N/A\"\n",
    "    if market_cap > 50_000_000_000:\n",
    "        return \"Blue Chip\"\n",
    "    if market_cap > 10_000_000_000:\n",
    "        return \"Mid Cap\"\n",
    "    if market_cap > 2_000_000_000:\n",
    "        return \"Small Cap\"\n",
    "    return \"Micro Cap\"\n",
    "\n",
    "def get_market_sentiment_and_details(ticker_obj: yf.Ticker) -> dict:\n",
    "    \"\"\"\n",
    "    Usa 'recommendations_summary' para maior robustez.\n",
    "    \"\"\"\n",
    "    sentiment_data = {\n",
    "        'Sentimento Gauge': 50.0, 'Strong Buy': 0, 'Buy': 0,\n",
    "        'Hold': 0, 'Sell': 0, 'Strong Sell': 0\n",
    "    }\n",
    "    try:\n",
    "        summary = ticker_obj.recommendations_summary\n",
    "        if summary is None or summary.empty:\n",
    "            return sentiment_data\n",
    "        rec_counts = summary.iloc[-1]\n",
    "        strong_buy = int(rec_counts.get('strongBuy', 0))\n",
    "        buy = int(rec_counts.get('buy', 0))\n",
    "        hold = int(rec_counts.get('hold', 0))\n",
    "        sell = int(rec_counts.get('sell', 0))\n",
    "        strong_sell = int(rec_counts.get('strongSell', 0))\n",
    "        sentiment_data.update({\n",
    "            'Strong Buy': strong_buy, 'Buy': buy, 'Hold': hold,\n",
    "            'Sell': sell, 'Strong Sell': strong_sell\n",
    "        })\n",
    "        weighted_score = (strong_buy * 2) + (buy * 1) + (hold * 0) + (sell * -1) + (strong_sell * -2)\n",
    "        total_recommendations = strong_buy + buy + hold + sell + strong_sell\n",
    "        if total_recommendations == 0:\n",
    "            return sentiment_data\n",
    "        avg_score = weighted_score / total_recommendations\n",
    "        normalized_sentiment = ((avg_score + 2) / 4) * 100\n",
    "        sentiment_data['Sentimento Gauge'] = normalized_sentiment\n",
    "        return sentiment_data\n",
    "    except Exception as e:\n",
    "        print(f\" (Aviso: Falha ao buscar sentimento - {e}) \", end=\"\")\n",
    "        return sentiment_data\n",
    "\n",
    "def fetch_stock_data(ticker: str, info_original: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Busca e processa os dados de um √∫nico ticker, com l√≥gica de fallback para Market Cap e DY 5 anos.\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    info = stock.info\n",
    "    history = stock.history(period=\"5y\")\n",
    "\n",
    "    market_cap = info.get('marketCap')\n",
    "    current_price = info.get(\"currentPrice\", 0.0)\n",
    "    if not market_cap or market_cap == 0:\n",
    "        shares_outstanding = info.get('sharesOutstanding', 0)\n",
    "        if shares_outstanding > 0 and current_price > 0:\n",
    "            market_cap = shares_outstanding * current_price\n",
    "    if not market_cap or market_cap == 0:\n",
    "        market_cap = info_original.get('Market Cap', 0)\n",
    "\n",
    "    # --- C√°lculos de Indicadores ---\n",
    "    # Dividend Yield 12 meses (j√° em percentual, apenas valida)\n",
    "    dy_12m = info.get(\"trailingAnnualDividendYield\", 0.0) * 100 if info.get(\"trailingAnnualDividendYield\") is not None else 0.0\n",
    "    if dy_12m > 50 or dy_12m < 0:\n",
    "        dy_12m = 0.0  # Prote√ß√£o contra valores absurdos\n",
    "\n",
    "    # Dividend Yield 5 anos (evitar multiplica√ß√£o por 100 e adicionar fallback)\n",
    "    dy_5y = info.get(\"fiveYearAvgDividendYield\", 0.0) if info.get(\"fiveYearAvgDividendYield\") is not None else 0.0\n",
    "    # Valida√ß√£o: valores fora de 0-50% s√£o considerados inv√°lidos\n",
    "    if dy_5y > 50 or dy_5y < 0:\n",
    "        print(f\" (Aviso: DY 5 anos de {ticker} ({dy_5y:.2f}%) inv√°lido, calculando manualmente...) \", end=\"\")\n",
    "        dy_5y = 0.0\n",
    "        try:\n",
    "            dividends = stock.dividends\n",
    "            if not dividends.empty:\n",
    "                # Soma os dividendos dos √∫ltimos 5 anos\n",
    "                dividends_5y = dividends[dividends.index >= (datetime.now() - pd.Timedelta(days=5*365))]\n",
    "                total_dividends = dividends_5y.sum()\n",
    "                # Calcula pre√ßo m√©dio dos √∫ltimos 5 anos\n",
    "                avg_price = history['Close'].mean() if not history.empty else current_price\n",
    "                if avg_price > 0:\n",
    "                    dy_5y = (total_dividends / 5) / avg_price * 100  # M√©dia anual de dividendos / pre√ßo m√©dio\n",
    "        except Exception as e:\n",
    "            print(f\" (Aviso: Falha ao calcular DY 5 anos manualmente para {ticker} - {e}) \", end=\"\")\n",
    "\n",
    "    growth_price = 0.0\n",
    "    if not history.empty and len(history['Close']) > 1 and history['Close'].iloc[0] > 0:\n",
    "        growth_price = ((history['Close'].iloc[-1] / history['Close'].iloc[0]) - 1) * 100\n",
    "    payout_ratio = info.get(\"payoutRatio\", 0.0) * 100 if info.get(\"payoutRatio\") is not None else 0.0\n",
    "    sentiment_info = get_market_sentiment_and_details(stock)\n",
    "\n",
    "    dados = {\n",
    "        \"Empresa\": info_original.get('Empresa', 'N/A'),\n",
    "        \"Setor (brapi)\": info_original.get('Setor (brapi)', 'N/A'),\n",
    "        \"Tipo\": info_original.get('Tipo', 'N/A'),\n",
    "        \"Market Cap\": market_cap,\n",
    "        \"Logo\": info_original.get('Logo', 'N/A'),\n",
    "        \"Pre√ßo Atual\": current_price,\n",
    "        \"P/L\": info.get(\"trailingPE\"),\n",
    "        \"P/VP\": info.get(\"priceToBook\"),\n",
    "        \"DY (Taxa 12m, %)\": dy_12m,\n",
    "        \"DY 5 Anos M√©dia (%)\": dy_5y,\n",
    "        \"√öltimo Dividendo\": info.get(\"lastDividendValue\"),\n",
    "        \"Data √ölt. Div.\": pd.to_datetime(info.get(\"lastDividendDate\"), unit='s', errors='coerce'),\n",
    "        \"Data Ex-Div.\": pd.to_datetime(info.get(\"exDividendDate\"), unit='s', errors='coerce'),\n",
    "        \"Payout Ratio (%)\": payout_ratio,\n",
    "        \"Crescimento Pre√ßo (%)\": growth_price,\n",
    "        \"ROE (%)\": info.get(\"returnOnEquity\", 0.0) * 100 if info.get(\"returnOnEquity\") is not None else 0.0,\n",
    "        \"D√≠vida Total\": info.get('totalDebt'),\n",
    "        \"EBITDA\": info.get('ebitda'),\n",
    "        \"Perfil da A√ß√£o\": classify_stock_profile(current_price, market_cap)\n",
    "    }\n",
    "    dados.update(sentiment_info)\n",
    "    if dados.get(\"EBITDA\") and dados[\"EBITDA\"] != 0 and dados.get(\"D√≠vida Total\"):\n",
    "        dados[\"D√≠vida/EBITDA\"] = dados[\"D√≠vida Total\"] / dados[\"EBITDA\"]\n",
    "    else:\n",
    "        dados[\"D√≠vida/EBITDA\"] = None\n",
    "    return dados\n",
    "\n",
    "def main():\n",
    "    arquivo_input = r\"E:\\Github\\Unicamp\\Bussola-de-Valor\\data\\scanner_acoes_e_fundos_filtrado.csv\"\n",
    "    output_path = r\"E:\\Github\\Unicamp\\Bussola-de-Valor\\data\\relatorio_analise_b3.csv\"\n",
    "    try:\n",
    "        df_input = pd.read_csv(arquivo_input)\n",
    "        tickers_para_analisar = df_input['Ticker'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERRO CR√çTICO: Arquivo de entrada '{arquivo_input}' n√£o encontrado.\")\n",
    "        return\n",
    "    dados_detalhados = {}\n",
    "    erros = []\n",
    "    print(\"\\nIniciando coleta de dados via yfinance...\")\n",
    "    for ticker in tickers_para_analisar:\n",
    "        print(f\"Processando: {ticker}...\", end=\"\")\n",
    "        try:\n",
    "            info_original = df_input[df_input['Ticker'] == ticker].iloc[0]\n",
    "            dados_detalhados[ticker] = fetch_stock_data(ticker, info_original)\n",
    "            print(\" ‚úÖ\")\n",
    "        except Exception as e:\n",
    "            erros.append(ticker)\n",
    "            print(f\" ‚ùå (Erro: {e})\")\n",
    "    if not dados_detalhados:\n",
    "        print(\"\\n‚ùå NENHUM DADO FOI COLETADO COM SUCESSO. O arquivo CSV n√£o ser√° gerado.\")\n",
    "        return\n",
    "    df_resultado = pd.DataFrame.from_dict(dados_detalhados, orient=\"index\")\n",
    "    df_resultado.fillna(0, inplace=True)\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        df_resultado.to_csv(output_path)\n",
    "        print(f\"\\n‚úÖ SUCESSO! Relat√≥rio salvo com sucesso em: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERRO INESPERADO AO SALVAR O ARQUIVO: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f857af",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # arquivo_input = r\"C:\\Users\\01701805\\Desktop\\Projetos GL\\finance-manager\\data\\scanner_acoes_e_fundos_filtrado.csv\"\n",
    "    # output_path = r\"C:\\Users\\01701805\\Desktop\\Projetos GL\\finance-manager\\data\\relatorio_analise_b3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py:\n",
    "\n",
    "# df = load_data(r\"C:\\Users\\01701805\\Desktop\\Projetos GL\\finance-manager\\data\\relatorio_analise_b3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df_dados = pd.read_excel(r'E:\\Github\\finance-manager\\datasets\\relatorio_completo_b3.xlsx')\n",
    "\n",
    "# print(df_dados.head(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
